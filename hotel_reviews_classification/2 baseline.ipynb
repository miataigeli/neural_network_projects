{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TkA4QiWdZTg1",
    "outputId": "8d9ac5ee-0821-430e-aadc-1bfac91dcf39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of the train set:  46867\n"
     ]
    }
   ],
   "source": [
    "## data exploration\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# vocab size of train set\n",
    "df = pd.read_table('/content/drive/MyDrive/data/train.tsv')\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "df = vectorizer.fit_transform(df['Review'])\n",
    "print('Vocab size of the train set: ', len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7N9blEyhgAXq"
   },
   "outputs": [],
   "source": [
    "# set seed\n",
    "import torch\n",
    "\n",
    "manual_seed = 572\n",
    "torch.manual_seed(manual_seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yc53D5RaVCPW"
   },
   "outputs": [],
   "source": [
    "## load and preprocess\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "# torchtext fields\n",
    "TEXT = Field(sequential=True, tokenize='spacy', lower=True) \n",
    "LABEL = Field(sequential=False, unk_token = None)\n",
    "\n",
    "# load data: train and val\n",
    "train, dev = TabularDataset.splits(\n",
    "    path='/content/drive/MyDrive/data', \n",
    "    train='train.tsv', validation='dev.tsv', \n",
    "    format='tsv',\n",
    "    skip_header=True, \n",
    "    fields=[('reviews', TEXT), ('ratings', LABEL)])\n",
    "\n",
    "# load data: test\n",
    "test = TabularDataset(\n",
    "  path=\"/content/drive/MyDrive/data/test.tsv\",\n",
    "  format='tsv',\n",
    "  skip_header=True, \n",
    "  fields=[('reviews', TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VGdQHJH5Xx4s"
   },
   "outputs": [],
   "source": [
    "## Baseline model\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "# import pretrained word embedding\n",
    "vectors = Vectors(name='glove.42B.300d.txt', cache='/content/drive/MyDrive/data')\n",
    "\n",
    "# build vocab, choose vocab size\n",
    "TEXT.build_vocab(train, max_size=5000, min_freq=3, vectors=vectors) ##### try: max_size = 5000, 10_000, 20_000, all; min_freq=3, 5, none\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "# create splits, choose batch size\n",
    "train_iter, dev_iter = BucketIterator.splits(\n",
    " (train, dev), \n",
    " batch_sizes=(32,32), ##### try: (32,32), (64,32), (64,32), (64,64)\n",
    " sort_key=lambda x: len(x.reviews), \n",
    " sort=True, \n",
    " sort_within_batch=True\n",
    ")\n",
    "\n",
    "test_iter = Iterator(\n",
    "  dataset = test, \n",
    "  sort = False, \n",
    "  batch_size = 32, ##### try: 32, 64\n",
    "  sort_key=None, \n",
    "  shuffle=False, \n",
    "  sort_within_batch=False, \n",
    "  device = device, \n",
    "  train=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FKfnC4pdadhC"
   },
   "outputs": [],
   "source": [
    "# create GRU class\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUmodel(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers, dropout_p, nonlin):\n",
    "        super(GRUmodel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size).from_pretrained(TEXT.vocab.vectors, freeze=False) ##### try: freeze = False, True\n",
    "        \n",
    "        self.gru_layer = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_p)\n",
    "        self.activation_fn = nonlin\n",
    "        self.linear_layer = nn.Linear(hidden_size, output_size) \n",
    "        self.softmax_layer = nn.LogSoftmax(dim=1)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out, _ = self.gru_layer(out)\n",
    "        out = out[-1, :,:]\n",
    "        out = self.activation_fn(out)\n",
    "        out = self.linear_layer(out)\n",
    "        out = self.softmax_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tIHT0g637aHI"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "HIDDEN_SIZE = 20 \n",
    "NUM_LAYERS = 2\n",
    "MAX_EPOCHS = 5 \n",
    "LEARNING_RATE = 0.01\n",
    "NUM_CLASSES = 5 \n",
    "EMBEDDING_SIZE = 300 \n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "DROPOUT_P = 0.01\n",
    "NONLIN = nn.ReLU()\n",
    "\n",
    "# set the seed\n",
    "manual_seed = 333\n",
    "torch.manual_seed(manual_seed)\n",
    "if n_gpu > 0:\n",
    "  torch.cuda.manual_seed(manual_seed)\n",
    "\n",
    "# set model, define loss, optimizer\n",
    "model = GRUmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_P, NONLIN)\n",
    "model.to(device) \n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pOsqUWRf_GOJ"
   },
   "outputs": [],
   "source": [
    "## train and evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_(loader):\n",
    "    total_loss = 0.0\n",
    "    num_sample = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        # load current batch\n",
    "        batch_input = batch.reviews\n",
    "        batch_output = batch.ratings\n",
    "        \n",
    "        batch_input = batch_input.to(device)\n",
    "        batch_output = batch_output.to(device)\n",
    "\n",
    "        # forward propagation\n",
    "        model_outputs = model(batch_input)\n",
    "        cur_loss = criterion(model_outputs, batch_output)\n",
    "        total_loss += cur_loss.cpu().item()\n",
    "\n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        cur_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        num_sample += batch_output.shape[0]\n",
    "\n",
    "    return total_loss/num_sample\n",
    "\n",
    "def evaluate(loader):\n",
    "    all_pred = []\n",
    "    all_label = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # load current batch\n",
    "            batch_input = batch.reviews\n",
    "            batch_output = batch.ratings\n",
    "\n",
    "            batch_input = batch_input.to(device)\n",
    "            \n",
    "            # forward propagation\n",
    "            model_outputs = model(batch_input)\n",
    "\n",
    "            # identify predicted class\n",
    "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
    "            all_pred.extend(predicted)\n",
    "            all_label.extend(batch_output.cpu())\n",
    "            \n",
    "    accuracy = accuracy_score(all_label, all_pred)\n",
    "    f1score = f1_score(all_label, all_pred, average='macro') \n",
    "    return accuracy,f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAxIrCRdBZk3",
    "outputId": "7088ae17-500d-44f7-8e43-8351cd1ccd33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0441, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n",
      "Epoch [2/5], Loss: 0.0424, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n",
      "Epoch [3/5], Loss: 0.0424, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n",
      "Epoch [4/5], Loss: 0.0424, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n",
      "Epoch [5/5], Loss: 0.0423, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n"
     ]
    }
   ],
   "source": [
    "# execute\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    train_loss = train_(train_iter)\n",
    "\n",
    "    train_acc, train_f1 = evaluate(train_iter)\n",
    "    val_acc, val_f1 = evaluate(dev_iter)\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fmHNVW6eCewj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
