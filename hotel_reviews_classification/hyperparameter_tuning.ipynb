{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperparameter_tuning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N9blEyhgAXq"
      },
      "source": [
        "# set seed\n",
        "import torch\n",
        "\n",
        "manual_seed = 572\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnZpaOJcI1ou"
      },
      "source": [
        "## vocab size of data\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vocab size of train set\n",
        "df = pd.read_table('/content/drive/MyDrive/data/train.tsv')\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "df = vectorizer.fit_transform(df['Review'])\n",
        "vocab_num = len(vectorizer.get_feature_names())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc53D5RaVCPW"
      },
      "source": [
        "## load and preprocess\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "# torchtext fields\n",
        "TEXT = Field(sequential=True, tokenize='spacy', lower=True) \n",
        "LABEL = Field(sequential=False, unk_token = None)\n",
        "\n",
        "# load data: train and val\n",
        "train, dev = TabularDataset.splits(\n",
        "    path='/content/drive/MyDrive/data', \n",
        "    train='train.tsv', validation='dev.tsv', \n",
        "    format='tsv',\n",
        "    skip_header=True, \n",
        "    fields=[('reviews', TEXT), ('ratings', LABEL)])\n",
        "\n",
        "# load data: test\n",
        "test = TabularDataset(\n",
        "  path=\"/content/drive/MyDrive/data/test.tsv\",\n",
        "  format='tsv',\n",
        "  skip_header=True, \n",
        "  fields=[('reviews', TEXT)])\n",
        "\n",
        "# word embedding\n",
        "vectors = Vectors(name='glove.42B.300d.txt', cache='/content/drive/MyDrive/data')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2uBkcaqFcAQ"
      },
      "source": [
        "# train and evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_(loader,model,criterion,optimizer,device):\n",
        "    total_loss = 0.0\n",
        "    num_sample = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        # load the current batch\n",
        "        batch_input = batch.reviews\n",
        "        batch_output = batch.ratings\n",
        "        \n",
        "        batch_input = batch_input.to(device)\n",
        "        batch_output = batch_output.to(device)\n",
        "\n",
        "        # forward propagation\n",
        "        model_outputs = model(batch_input)\n",
        "        cur_loss = criterion(model_outputs, batch_output)\n",
        "        total_loss += cur_loss.cpu().item()\n",
        "\n",
        "        # backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        cur_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        num_sample += batch_output.shape[0]\n",
        "    return total_loss/num_sample\n",
        "\n",
        "def evaluate(loader,model,criterion,device):\n",
        "    all_pred=[]\n",
        "    all_label = []\n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        for batch in loader:\n",
        "             # load the current batch\n",
        "            batch_input = batch.reviews\n",
        "            batch_output = batch.ratings\n",
        "\n",
        "            batch_input = batch_input.to(device)\n",
        "\n",
        "            # forward propagation\n",
        "            model_outputs = model(batch_input)\n",
        "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(batch_output.cpu())\n",
        "            \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return accuracy, f1score"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKfnC4pdadhC"
      },
      "source": [
        "# create neural network\n",
        "import torch.nn as nn\n",
        "\n",
        "class GRUmodel(nn.Module):\n",
        "    def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers, dropout_p, nonlin):\n",
        "        super(GRUmodel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size).from_pretrained(TEXT.vocab.vectors, freeze=False)\n",
        "        \n",
        "        self.gru_layer = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_p)\n",
        "        self.activation_fn = nonlin\n",
        "        self.linear_layer = nn.Linear(hidden_size, output_size) \n",
        "        self.softmax_layer = nn.LogSoftmax(dim=1)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out, _ = self.gru_layer(out)\n",
        "        out = out[-1, :,:]\n",
        "        out = self.activation_fn(out)\n",
        "        out = self.linear_layer(out)\n",
        "        out = self.softmax_layer(out)\n",
        "        return out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q58O4t8BFMXO"
      },
      "source": [
        "## hyperparameter tuning\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "import scipy\n",
        "import numpy as np\n",
        "\n",
        "def random_search(num_iter):\n",
        "    for i in range(num_iter):\n",
        "        results = []\n",
        "        config = {\n",
        "            'max_vocab': scipy.stats.randint.rvs(5000, vocab_num),\n",
        "            'min_freq': scipy.stats.randint.rvs(0,5),\n",
        "            'batch_size': np.random.choice([32, 64]),\n",
        "            'layers': scipy.stats.randint.rvs(2,6),\n",
        "            'lr': scipy.stats.loguniform.rvs(10**-3,1),\n",
        "            'nonlin' : np.random.choice([nn.ReLU(),nn.Tanh(), nn.Sigmoid()]),\n",
        "            'dropout': scipy.stats.loguniform.rvs(0.01,0.9),\n",
        "            'hidden_nodes': scipy.stats.randint.rvs(50,300),\n",
        "            'max_epochs': scipy.stats.randint.rvs(5,20),\n",
        "            'momentum': np.random.choice([0.99, 0.9, 0.5])\n",
        "        }\n",
        "        \n",
        "        # build vocab, choose vocab size\n",
        "        TEXT.build_vocab(train, max_size=config['max_vocab'], min_freq=config['min_freq'], vectors=vectors)\n",
        "        LABEL.build_vocab(train)\n",
        "\n",
        "        # create splits, choose batch size\n",
        "        train_iter, dev_iter = BucketIterator.splits(\n",
        "        (train, dev), \n",
        "        batch_sizes=(config['batch_size'], config['batch_size']),\n",
        "        sort_key=lambda x: len(x.reviews), \n",
        "        sort=True, \n",
        "        sort_within_batch=True\n",
        "        )\n",
        "\n",
        "        test_iter = Iterator(\n",
        "          dataset = test, \n",
        "          sort = False, \n",
        "          batch_size = config['batch_size'],\n",
        "          sort_key=None, \n",
        "          shuffle=False, \n",
        "          sort_within_batch=False, \n",
        "          device = device, \n",
        "          train=False \n",
        "        )\n",
        "\n",
        "        # constants\n",
        "        NUM_CLASSES = 5 \n",
        "        EMBEDDING_SIZE = 300 \n",
        "        VOCAB_SIZE = len(TEXT.vocab)\n",
        "\n",
        "        # initialize model\n",
        "        model = GRUmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, config['hidden_nodes'], config['layers'], config['dropout'], config['nonlin'])\n",
        "        model.to(device)\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = np.random.choice([torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum']), \n",
        "                                      torch.optim.Adam(model.parameters(), lr=config['lr'])])\n",
        "        \n",
        "        # print configuration\n",
        "        print(\"new config, iteration\", i+1)\n",
        "        config['optimizer'] = optimizer\n",
        "        print(config)\n",
        "\n",
        "        max_dev = 0\n",
        "        best_epoch = 0\n",
        "        for epoch in range(config['max_epochs']):\n",
        "            train_loss = train_(train_iter,model,criterion,optimizer,device)\n",
        "\n",
        "            train_acc, train_f1 = evaluate(train_iter,model,criterion,device)\n",
        "            dev_acc, dev_f1 = evaluate(dev_iter,model,criterion,device)\n",
        "                    \n",
        "            if dev_acc > max_dev:\n",
        "                max_dev = dev_acc\n",
        "                best_epoch = epoch+1\n",
        "                \n",
        "            print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, config['max_epochs'], train_loss, train_acc, dev_acc))\n",
        "        results.append((max_dev,best_epoch,config))\n",
        "        print(\"Best validation score for iterations #{}: {}\".format(i+1,max_dev))\n",
        "    return results"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhsF7y3ZPLR_",
        "outputId": "647b7c7d-b535-425a-cc69-ec2e6d9a67ea"
      },
      "source": [
        "random_search(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new config, iteration 1\n",
            "{'max_vocab': 41042, 'min_freq': 0, 'batch_size': 64, 'layers': 3, 'lr': 0.12227144096824473, 'nonlin': Tanh(), 'dropout': 0.7163376083095391, 'hidden_nodes': 179, 'max_epochs': 16, 'momentum': 0.99, 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.12227144096824473\n",
            "    momentum: 0.99\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")}\n",
            "Epoch [1/16], Loss: 0.0820, Training Accuracy: 0.2999, Validation Accuracy: 0.3255\n",
            "Epoch [2/16], Loss: 0.1810, Training Accuracy: 0.2604, Validation Accuracy: 0.2733\n",
            "Epoch [3/16], Loss: 0.2379, Training Accuracy: 0.3168, Validation Accuracy: 0.3236\n",
            "Epoch [4/16], Loss: 0.2288, Training Accuracy: 0.2945, Validation Accuracy: 0.3050\n",
            "Epoch [5/16], Loss: 0.2091, Training Accuracy: 0.1416, Validation Accuracy: 0.1415\n",
            "Epoch [6/16], Loss: 0.1847, Training Accuracy: 0.2681, Validation Accuracy: 0.2714\n",
            "Epoch [7/16], Loss: 0.1877, Training Accuracy: 0.3187, Validation Accuracy: 0.3324\n",
            "Epoch [8/16], Loss: 0.1592, Training Accuracy: 0.2760, Validation Accuracy: 0.2704\n",
            "Epoch [9/16], Loss: 0.1950, Training Accuracy: 0.3274, Validation Accuracy: 0.3372\n",
            "Epoch [10/16], Loss: 0.1723, Training Accuracy: 0.1623, Validation Accuracy: 0.1879\n",
            "Epoch [11/16], Loss: 0.2145, Training Accuracy: 0.2643, Validation Accuracy: 0.2684\n",
            "Epoch [12/16], Loss: 0.2069, Training Accuracy: 0.2942, Validation Accuracy: 0.2948\n",
            "Epoch [13/16], Loss: 0.1786, Training Accuracy: 0.2818, Validation Accuracy: 0.2836\n",
            "Epoch [14/16], Loss: 0.1841, Training Accuracy: 0.2749, Validation Accuracy: 0.2792\n",
            "Epoch [15/16], Loss: 0.1667, Training Accuracy: 0.2628, Validation Accuracy: 0.2435\n",
            "Epoch [16/16], Loss: 0.1988, Training Accuracy: 0.2196, Validation Accuracy: 0.2143\n",
            "Best validation score for iterations #1: 0.33723767691556855\n",
            "new config, iteration 2\n",
            "{'max_vocab': 28868, 'min_freq': 0, 'batch_size': 64, 'layers': 4, 'lr': 0.002996653929554983, 'nonlin': Sigmoid(), 'dropout': 0.012996326587680962, 'hidden_nodes': 142, 'max_epochs': 8, 'momentum': 0.9, 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.002996653929554983\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")}\n",
            "Epoch [1/8], Loss: 0.0213, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [2/8], Loss: 0.0212, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [3/8], Loss: 0.0212, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [4/8], Loss: 0.0212, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [5/8], Loss: 0.0212, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [6/8], Loss: 0.0212, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [7/8], Loss: 0.0212, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [8/8], Loss: 0.0212, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Best validation score for iterations #2: 0.2957540263543192\n",
            "new config, iteration 3\n",
            "{'max_vocab': 22558, 'min_freq': 0, 'batch_size': 32, 'layers': 2, 'lr': 0.0034072340872709534, 'nonlin': Sigmoid(), 'dropout': 0.5191402611905828, 'hidden_nodes': 99, 'max_epochs': 10, 'momentum': 0.99, 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.0034072340872709534\n",
            "    momentum: 0.99\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")}\n",
            "Epoch [1/10], Loss: 0.0438, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [2/10], Loss: 0.0430, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [3/10], Loss: 0.0428, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [4/10], Loss: 0.0428, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [5/10], Loss: 0.0427, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [6/10], Loss: 0.0412, Training Accuracy: 0.3782, Validation Accuracy: 0.3616\n",
            "Epoch [7/10], Loss: 0.0355, Training Accuracy: 0.4585, Validation Accuracy: 0.4636\n",
            "Epoch [8/10], Loss: 0.0305, Training Accuracy: 0.5281, Validation Accuracy: 0.5144\n",
            "Epoch [9/10], Loss: 0.0286, Training Accuracy: 0.5429, Validation Accuracy: 0.5237\n",
            "Epoch [10/10], Loss: 0.0262, Training Accuracy: 0.5913, Validation Accuracy: 0.5661\n",
            "Best validation score for iterations #3: 0.5661298194241093\n",
            "new config, iteration 4\n",
            "{'max_vocab': 39523, 'min_freq': 2, 'batch_size': 32, 'layers': 4, 'lr': 0.7074242442935745, 'nonlin': Sigmoid(), 'dropout': 0.012031097189299131, 'hidden_nodes': 60, 'max_epochs': 19, 'momentum': 0.5, 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.7074242442935745\n",
            "    momentum: 0.5\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")}\n",
            "Epoch [1/19], Loss: 0.0447, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [2/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [3/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [4/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [5/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [6/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [7/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [8/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [9/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [10/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [11/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [12/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [13/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [14/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [15/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [16/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [17/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [18/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [19/19], Loss: 0.0443, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Best validation score for iterations #4: 0.2957540263543192\n",
            "new config, iteration 5\n",
            "{'max_vocab': 10258, 'min_freq': 4, 'batch_size': 64, 'layers': 5, 'lr': 0.6134416905016794, 'nonlin': Tanh(), 'dropout': 0.04835881755048637, 'hidden_nodes': 243, 'max_epochs': 5, 'momentum': 0.99, 'optimizer': Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.6134416905016794\n",
            "    weight_decay: 0\n",
            ")}\n",
            "Epoch [1/5], Loss: 0.2173, Training Accuracy: 0.2971, Validation Accuracy: 0.2967\n",
            "Epoch [2/5], Loss: 0.2240, Training Accuracy: 0.3058, Validation Accuracy: 0.2977\n",
            "Epoch [3/5], Loss: 0.1723, Training Accuracy: 0.3067, Validation Accuracy: 0.3006\n",
            "Epoch [4/5], Loss: 0.1856, Training Accuracy: 0.1776, Validation Accuracy: 0.1776\n",
            "Epoch [5/5], Loss: 0.1929, Training Accuracy: 0.2942, Validation Accuracy: 0.2967\n",
            "Best validation score for iterations #5: 0.30063445583211323\n",
            "new config, iteration 6\n",
            "{'max_vocab': 28392, 'min_freq': 0, 'batch_size': 64, 'layers': 3, 'lr': 0.7824879120675491, 'nonlin': ReLU(), 'dropout': 0.3283594472040188, 'hidden_nodes': 69, 'max_epochs': 5, 'momentum': 0.99, 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.7824879120675491\n",
            "    momentum: 0.99\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")}\n",
            "Epoch [1/5], Loss: 0.0573, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [2/5], Loss: 0.1251, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n",
            "Epoch [3/5], Loss: 0.2128, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n",
            "Epoch [4/5], Loss: 0.2724, Training Accuracy: 0.0868, Validation Accuracy: 0.0927\n",
            "Epoch [5/5], Loss: 0.3459, Training Accuracy: 0.4427, Validation Accuracy: 0.4305\n",
            "Best validation score for iterations #6: 0.43045387994143486\n",
            "new config, iteration 7\n",
            "{'max_vocab': 13993, 'min_freq': 0, 'batch_size': 32, 'layers': 5, 'lr': 0.03861552224902284, 'nonlin': Sigmoid(), 'dropout': 0.01736264691017661, 'hidden_nodes': 286, 'max_epochs': 11, 'momentum': 0.99, 'optimizer': SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.03861552224902284\n",
            "    momentum: 0.99\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")}\n",
            "Epoch [1/11], Loss: 0.0591, Training Accuracy: 0.1055, Validation Accuracy: 0.1171\n",
            "Epoch [2/11], Loss: 0.0691, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [3/11], Loss: 0.0803, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [4/11], Loss: 0.0627, Training Accuracy: 0.1055, Validation Accuracy: 0.1171\n",
            "Epoch [5/11], Loss: 0.0688, Training Accuracy: 0.2944, Validation Accuracy: 0.2958\n",
            "Epoch [6/11], Loss: 0.0526, Training Accuracy: 0.1055, Validation Accuracy: 0.1171\n",
            "Epoch [7/11], Loss: 0.0779, Training Accuracy: 0.0706, Validation Accuracy: 0.0639\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}