{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkA4QiWdZTg1"
      },
      "source": [
        "# ## data exploration\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# # vocab size of train set\n",
        "# df = pd.read_table('/content/drive/MyDrive/data/train.tsv')\n",
        "# vectorizer = CountVectorizer()\n",
        "\n",
        "# df = vectorizer.fit_transform(df['Review'])\n",
        "# print('Vocab size of the train set: ', len(vectorizer.get_feature_names()))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N9blEyhgAXq"
      },
      "source": [
        "# set seed\n",
        "import torch\n",
        "\n",
        "manual_seed = 572\n",
        "torch.manual_seed(manual_seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc53D5RaVCPW"
      },
      "source": [
        "## load and preprocess\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "\n",
        "# torchtext fields\n",
        "TEXT = Field(sequential=True, tokenize='spacy', lower=True) \n",
        "LABEL = Field(sequential=False, unk_token = None)\n",
        "\n",
        "# load data: train and val\n",
        "train, dev = TabularDataset.splits(\n",
        "    path='/content/drive/MyDrive/data', \n",
        "    train='train.tsv', validation='dev.tsv', \n",
        "    format='tsv',\n",
        "    skip_header=True, \n",
        "    fields=[('reviews', TEXT), ('ratings', LABEL)])\n",
        "\n",
        "# load data: test\n",
        "test = TabularDataset(\n",
        "  path=\"/content/drive/MyDrive/data/test.tsv\",\n",
        "  format='tsv',\n",
        "  skip_header=True, \n",
        "  fields=[('reviews', TEXT)])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGdQHJH5Xx4s"
      },
      "source": [
        "## Baseline model\n",
        "from torchtext.vocab import Vectors\n",
        "from torchtext.data import Iterator, BucketIterator\n",
        "\n",
        "# import pretrained word embedding\n",
        "vectors = Vectors(name='glove.42B.300d.txt', cache='/content/drive/MyDrive/data')\n",
        "\n",
        "# build vocab, choose vocab size\n",
        "TEXT.build_vocab(train, max_size=5000, min_freq=3, vectors=vectors) ##### try: max_size = 5000, 10_000, 20_000, all; min_freq=3, 5, none\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "# create splits, choose batch size\n",
        "train_iter, dev_iter = BucketIterator.splits(\n",
        " (train, dev), \n",
        " batch_sizes=(32,32), ##### try: (32,32), (64,32), (64,32), (64,64)\n",
        " sort_key=lambda x: len(x.reviews), \n",
        " sort=True, \n",
        " sort_within_batch=True\n",
        ")\n",
        "\n",
        "test_iter = Iterator(\n",
        "  dataset = test, \n",
        "  sort = False, \n",
        "  batch_size = 32, ##### try: 32, 64\n",
        "  sort_key=None, \n",
        "  shuffle=False, \n",
        "  sort_within_batch=False, \n",
        "  device = device, \n",
        "  train=False \n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKfnC4pdadhC"
      },
      "source": [
        "# create GRU class\n",
        "import torch.nn as nn\n",
        "\n",
        "class GRUmodel(nn.Module):\n",
        "    def __init__(self, embedding_size, vocab_size, output_size, hidden_size, num_layers, dropout_p, nonlin):\n",
        "        super(GRUmodel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size).from_pretrained(TEXT.vocab.vectors, freeze=False) ##### try: freeze = False, True\n",
        "        \n",
        "        self.gru_layer = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout_p)\n",
        "        self.activation_fn = nonlin\n",
        "        self.linear_layer = nn.Linear(hidden_size, output_size) \n",
        "        self.softmax_layer = nn.LogSoftmax(dim=1)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out, _ = self.gru_layer(out)\n",
        "        out = out[-1, :,:]\n",
        "        out = self.activation_fn(out)\n",
        "        out = self.linear_layer(out)\n",
        "        out = self.softmax_layer(out)\n",
        "        return out"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIHT0g637aHI"
      },
      "source": [
        "# hyperparameters\n",
        "HIDDEN_SIZE = 207 \n",
        "NUM_LAYERS = 2\n",
        "MAX_EPOCHS = 20 \n",
        "LEARNING_RATE = 0.00413267391256542\n",
        "NUM_CLASSES = 5 \n",
        "EMBEDDING_SIZE = 300 \n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "DROPOUT_P = 0.023313434837778705\n",
        "MOMENTUM = 0.99\n",
        "NONLIN = nn.ReLU()\n",
        "\n",
        "# set the seed\n",
        "manual_seed = 333\n",
        "torch.manual_seed(manual_seed)\n",
        "if n_gpu > 0:\n",
        "  torch.cuda.manual_seed(manual_seed)\n",
        "\n",
        "# set model, define loss, optimizer\n",
        "model = GRUmodel(EMBEDDING_SIZE, VOCAB_SIZE, NUM_CLASSES, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_P, NONLIN)\n",
        "model.to(device) \n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "criterion.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOsqUWRf_GOJ"
      },
      "source": [
        "## train and evaluate\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def train_(loader):\n",
        "    total_loss = 0.0\n",
        "    num_sample = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        # load current batch\n",
        "        batch_input = batch.reviews\n",
        "        batch_output = batch.ratings\n",
        "        \n",
        "        batch_input = batch_input.to(device)\n",
        "        batch_output = batch_output.to(device)\n",
        "\n",
        "        # forward propagation\n",
        "        model_outputs = model(batch_input)\n",
        "        cur_loss = criterion(model_outputs, batch_output)\n",
        "        total_loss += cur_loss.cpu().item()\n",
        "\n",
        "        # backward propagation\n",
        "        optimizer.zero_grad()\n",
        "        cur_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        num_sample += batch_output.shape[0]\n",
        "\n",
        "    return total_loss/num_sample\n",
        "\n",
        "def evaluate(loader):\n",
        "    all_pred = []\n",
        "    all_label = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            # load current batch\n",
        "            batch_input = batch.reviews\n",
        "            batch_output = batch.ratings\n",
        "\n",
        "            batch_input = batch_input.to(device)\n",
        "            \n",
        "            # forward propagation\n",
        "            model_outputs = model(batch_input)\n",
        "\n",
        "            # identify predicted class\n",
        "            probabilities, predicted = torch.max(model_outputs.cpu().data, 1)\n",
        "            all_pred.extend(predicted)\n",
        "            all_label.extend(batch_output.cpu())\n",
        "            \n",
        "    accuracy = accuracy_score(all_label, all_pred)\n",
        "    f1score = f1_score(all_label, all_pred, average='macro') \n",
        "    return accuracy,f1score"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAxIrCRdBZk3",
        "outputId": "079b5804-0195-4d4b-ec31-58948b56228b"
      },
      "source": [
        "# execute\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "\n",
        "    train_loss = train_(train_iter)\n",
        "\n",
        "    train_acc, train_f1 = evaluate(train_iter)\n",
        "    val_acc, val_f1 = evaluate(dev_iter)\n",
        "    \n",
        "    print('Epoch [{}/{}], Loss: {:.4f}, Training Accuracy: {:.4f}, Validation Accuracy: {:.4f}'.format(epoch+1, MAX_EPOCHS, train_loss, train_acc, val_acc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 0.0421, Training Accuracy: 0.2732, Validation Accuracy: 0.2914\n",
            "Epoch [2/20], Loss: 0.0346, Training Accuracy: 0.5298, Validation Accuracy: 0.5154\n",
            "Epoch [3/20], Loss: 0.0302, Training Accuracy: 0.5165, Validation Accuracy: 0.5095\n",
            "Epoch [4/20], Loss: 0.0276, Training Accuracy: 0.5977, Validation Accuracy: 0.5852\n",
            "Epoch [5/20], Loss: 0.0257, Training Accuracy: 0.6388, Validation Accuracy: 0.6110\n",
            "Epoch [6/20], Loss: 0.0249, Training Accuracy: 0.6382, Validation Accuracy: 0.5988\n",
            "Epoch [7/20], Loss: 0.0244, Training Accuracy: 0.6753, Validation Accuracy: 0.6418\n",
            "Epoch [8/20], Loss: 0.0234, Training Accuracy: 0.6690, Validation Accuracy: 0.6432\n",
            "Epoch [9/20], Loss: 0.0226, Training Accuracy: 0.6308, Validation Accuracy: 0.5827\n",
            "Epoch [10/20], Loss: 0.0232, Training Accuracy: 0.6840, Validation Accuracy: 0.6481\n",
            "Epoch [11/20], Loss: 0.0221, Training Accuracy: 0.6510, Validation Accuracy: 0.6271\n",
            "Epoch [12/20], Loss: 0.0227, Training Accuracy: 0.6834, Validation Accuracy: 0.6252\n",
            "Epoch [13/20], Loss: 0.0217, Training Accuracy: 0.6246, Validation Accuracy: 0.5842\n",
            "Epoch [14/20], Loss: 0.0217, Training Accuracy: 0.7102, Validation Accuracy: 0.6384\n",
            "Epoch [15/20], Loss: 0.0202, Training Accuracy: 0.7016, Validation Accuracy: 0.6262\n",
            "Epoch [16/20], Loss: 0.0204, Training Accuracy: 0.7254, Validation Accuracy: 0.6359\n",
            "Epoch [17/20], Loss: 0.0202, Training Accuracy: 0.7365, Validation Accuracy: 0.6496\n",
            "Epoch [18/20], Loss: 0.0199, Training Accuracy: 0.7317, Validation Accuracy: 0.6457\n",
            "Epoch [19/20], Loss: 0.0194, Training Accuracy: 0.5729, Validation Accuracy: 0.5251\n",
            "Epoch [20/20], Loss: 0.0199, Training Accuracy: 0.6063, Validation Accuracy: 0.5207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmHNVW6eCewj"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}